import cv2
import os
import numpy as np
import random
import glob
import shutil
from tqdm import tqdm
from sklearn.cluster import KMeans
from collections import defaultdict

#Path settings
DATA_ROOT = "Data"
NO_DUST_IMAGES_DIR = "Data/no_dust"
OUTPUT_DIR = "Data/synthetic_data"

# ========== ä¼˜åŒ–å‚æ•°é…ç½® ==========
# ç­–ç•¥é€‰æ‹©
USE_BRIGHTNESS_MATCHING = True      # å¯ç”¨äº®åº¦åŸŸåŒ¹é…
USE_SPATIAL_ANCHORING = True        # å¯ç”¨ç©ºé—´ä½ç½®é”šå®š
USE_BACKGROUND_CLUSTERING = True    # å¯ç”¨èƒŒæ™¯èšç±»
USE_RESIDUAL_FUSION = False         # å¯ç”¨æ®‹å·®èåˆæ¨¡å¼ï¼ˆéœ€è¦æ¨¡æ¿å›¾ï¼‰

# å‚æ•°è®¾ç½®
BRIGHTNESS_THRESHOLD = 15           # äº®åº¦å·®å¼‚å®¹å¿é˜ˆå€¼ï¼ˆ0-255ï¼‰
SPATIAL_RADIUS_RATIO = 0.3          # ç©ºé—´é”šå®šåŠå¾„ï¼ˆç›¸å¯¹å›¾åƒå®½åº¦ï¼‰
NUM_BG_CLUSTERS = 3                 # èƒŒæ™¯èšç±»æ•°é‡ï¼ˆäº®ã€ä¸­ã€æš—ï¼‰
RESIDUAL_ALPHA = 0.7                # æ®‹å·®èåˆå¼ºåº¦

# Clean and recreate output directories to avoid mixing old and new data
OUTPUT_IMAGES_DIR = os.path.join(OUTPUT_DIR, "images")
OUTPUT_LABELS_DIR = os.path.join(OUTPUT_DIR, "labels")

if os.path.exists(OUTPUT_IMAGES_DIR):
    print(f"æ¸…ç†æ—§æ•°æ®: {OUTPUT_IMAGES_DIR}...")
    shutil.rmtree(OUTPUT_IMAGES_DIR)
if os.path.exists(OUTPUT_LABELS_DIR):
    print(f"æ¸…ç†æ—§æ•°æ®: {OUTPUT_LABELS_DIR}...")
    shutil.rmtree(OUTPUT_LABELS_DIR)

os.makedirs(OUTPUT_IMAGES_DIR, exist_ok=True)
os.makedirs(OUTPUT_LABELS_DIR, exist_ok=True)

def xywhn2xyxy(x, y, w, h, W, H):
    """Convert normalized xywh center format to pixel xyxy format"""
    x1 = int((x - w / 2) * W)
    y1 = int((y - h / 2) * H)
    x2 = int((x + w / 2) * W)
    y2 = int((y + h / 2) * H)
    
    # Clip coordinates
    x1 = max(0, x1)
    y1 = max(0, y1)
    x2 = min(W, x2)
    y2 = min(H, y2)
    
    return x1, y1, x2, y2

def compute_brightness(img):
    """è®¡ç®—å›¾åƒçš„å¹³å‡äº®åº¦"""
    if len(img.shape) == 3:
        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    else:
        gray = img
    return np.mean(gray)

def extract_background_features(img, x1, y1, x2, y2, border=5):
    """æå–ç¼ºé™·å‘¨å›´èƒŒæ™¯åŒºåŸŸçš„ç‰¹å¾ï¼ˆç”¨äºäº®åº¦åŒ¹é…ï¼‰"""
    H, W = img.shape[:2]
    # æ‰©å±•è¾¹ç•Œä»¥è·å–å‘¨å›´èƒŒæ™¯
    bg_x1 = max(0, x1 - border)
    bg_y1 = max(0, y1 - border)
    bg_x2 = min(W, x2 + border)
    bg_y2 = min(H, y2 + border)
    
    bg_region = img[bg_y1:bg_y2, bg_x1:bg_x2]
    
    # è®¡ç®—èƒŒæ™¯äº®åº¦ï¼ˆæ’é™¤ç¼ºé™·æœ¬èº«ï¼‰
    mask = np.ones(bg_region.shape[:2], dtype=bool)
    inner_y1 = y1 - bg_y1
    inner_x1 = x1 - bg_x1
    inner_y2 = inner_y1 + (y2 - y1)
    inner_x2 = inner_x1 + (x2 - x1)
    mask[inner_y1:inner_y2, inner_x1:inner_x2] = False
    
    if len(bg_region.shape) == 3:
        gray_bg = cv2.cvtColor(bg_region, cv2.COLOR_BGR2GRAY)
    else:
        gray_bg = bg_region
    
    if mask.sum() > 0:
        brightness = np.mean(gray_bg[mask])
    else:
        brightness = np.mean(gray_bg)
    
    return brightness

def cv_imread(file_path):
    """Read image with unicode path support, forcing BGR"""
    try:
        cv_img = cv2.imdecode(np.fromfile(file_path, dtype=np.uint8), 1)
        return cv_img
    except Exception as e:
        print(f"Error reading {file_path}: {e}")
        return None

def cv_imwrite(file_path, img):
    """Write image with unicode path support"""
    try:
        cv2.imencode(os.path.splitext(file_path)[1], img)[1].tofile(file_path)
        return True
    except Exception as e:
        print(f"Error writing {file_path}: {e}")
        return False

def load_dust_samples_and_stats():
    """æå–æ‰€æœ‰ç°å°˜æ ·æœ¬ï¼Œå¹¶è®°å½•ä½ç½®ã€äº®åº¦ã€èƒŒæ™¯ç‰¹å¾ç­‰å…ƒä¿¡æ¯"""
    dust_metadata = []
    
    print("[ä¼˜åŒ–æ¨¡å¼] æå–ç°å°˜æ ·æœ¬åŠä¸Šä¸‹æ–‡ç‰¹å¾...")
    
    # Save debug patches to verify what we are cropping
    debug_patch_dir = os.path.join(OUTPUT_DIR, "debug_patches")
    os.makedirs(debug_patch_dir, exist_ok=True)
    
    splits = ['train', 'val', 'test']
    
    for split in splits:
        images_dir = os.path.join(DATA_ROOT, "images", split)
        labels_dir = os.path.join(DATA_ROOT, "labels", split)
        
        if not os.path.exists(labels_dir):
            print(f"è·³è¿‡ {split}: æœªæ‰¾åˆ°æ ‡ç­¾ç›®å½• {labels_dir}")
            continue
            
        label_files = glob.glob(os.path.join(labels_dir, "*.txt"))
        print(f"å¤„ç† {split}: å‘ç° {len(label_files)} ä¸ªæ ‡ç­¾æ–‡ä»¶")
        
        for label_file in tqdm(label_files, desc=f"åŠ è½½ {split}"):
            # Find corresponding image
            basename = os.path.splitext(os.path.basename(label_file))[0]
            img_found = False
            img_path = ""
            for ext in ['.jpg', '.png', '.jpeg', '.JPG', '.PNG', '.bmp']:
                temp_path = os.path.join(images_dir, basename + ext)
                if os.path.exists(temp_path):
                    img_path = temp_path
                    img_found = True
                    break
            
            if not img_found:
                continue
                
            img = cv_imread(img_path)
            if img is None:
                continue
                
            H, W = img.shape[:2]
            
            with open(label_file, 'r') as f:
                lines = f.readlines()
                
            for line in lines:
                parts = line.strip().split()
                if len(parts) < 5:
                    continue
                
                # Assume class 0 is dust
                x, y, w, h = map(float, parts[1:])
                x1, y1, x2, y2 = xywhn2xyxy(x, y, w, h, W, H)
                
                # Skip invalid small boxes
                if x2 - x1 < 5 or y2 - y1 < 5:
                    continue
                    
                patch = img[y1:y2, x1:x2]
                if patch.size == 0:
                    continue
                
                # æå–èƒŒæ™¯äº®åº¦ç‰¹å¾
                bg_brightness = extract_background_features(img, x1, y1, x2, y2, border=5)
                
                # ä¿å­˜å®Œæ•´å…ƒæ•°æ®
                dust_metadata.append({
                    'patch': patch.copy(),
                    'position': (x, y),
                    'brightness': bg_brightness,
                    'abs_position': (x1, y1, x2, y2),
                    'img_shape': (H, W)
                })
                
                # Save first 150 patches for debugging
                if len(dust_metadata) <= 150:
                    cv_imwrite(os.path.join(debug_patch_dir, 
                                           f"patch_{split}_{len(dust_metadata)}_br{int(bg_brightness)}.jpg"), 
                              patch)
            
    dust_brightness_list = [m['brightness'] for m in dust_metadata]
    if dust_brightness_list:
        print(f"âœ“ æå– {len(dust_metadata)} ä¸ªç°å°˜æ ·æœ¬")
        print(f"  äº®åº¦èŒƒå›´: [{min(dust_brightness_list):.1f}, {max(dust_brightness_list):.1f}]")
        print(f"  è°ƒè¯•å›¾åƒ: {debug_patch_dir}")
    return dust_metadata

def create_soft_mask(patch_h, patch_w, feather_size=5):
    """Create a feathered alpha mask for smooth blending"""
    mask = np.zeros((patch_h, patch_w), dtype=np.float32)
    
    # Ensure feather size doesn't exceed patch dimensions
    feather_size = min(feather_size, patch_h // 2, patch_w // 2)
    
    if feather_size <= 0:
        return np.ones((patch_h, patch_w), dtype=np.float32)
    
    # Inner region is 1.0 (fully opaque)
    mask[feather_size:patch_h-feather_size, feather_size:patch_w-feather_size] = 1.0
    
    # Apply Gaussian blur to create soft edges
    ksize = 2 * feather_size + 1
    if ksize % 2 == 0:
        ksize += 1
    mask = cv2.GaussianBlur(mask, (ksize, ksize), 0)
    
    # Normalize to [0, 1]
    if mask.max() > 0:
        mask = mask / mask.max()
    
    return mask

def cluster_backgrounds(bg_images, n_clusters=3):
    """å¯¹èƒŒæ™¯å›¾åƒè¿›è¡Œèšç±»ï¼ˆåŸºäºäº®åº¦å’Œçº¹ç†ç‰¹å¾ï¼‰"""
    print(f"\n[èƒŒæ™¯èšç±»] åˆ†æ {len(bg_images)} å¼ èƒŒæ™¯å›¾åƒ...")
    
    features = []
    valid_images = []
    
    for bg_path in tqdm(bg_images, desc="æå–èƒŒæ™¯ç‰¹å¾"):
        img = cv_imread(bg_path)
        if img is None:
            continue
            
        # ç‰¹å¾1: å¹³å‡äº®åº¦
        brightness = compute_brightness(img)
        
        # ç‰¹å¾2: äº®åº¦æ ‡å‡†å·®ï¼ˆçº¹ç†å¤æ‚åº¦ï¼‰
        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) if len(img.shape) == 3 else img
        std_dev = np.std(gray)
        
        # ç‰¹å¾3: è¾¹ç¼˜å¯†åº¦ï¼ˆä½¿ç”¨Cannyï¼‰
        edges = cv2.Canny(gray, 50, 150)
        edge_density = np.sum(edges > 0) / edges.size
        
        features.append([brightness, std_dev, edge_density * 100])
        valid_images.append(bg_path)
    
    features = np.array(features)
    
    # K-means èšç±»
    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
    labels = kmeans.fit_predict(features)
    
    # æŒ‰ç±»åˆ«ç»„ç»‡
    bg_clusters = defaultdict(list)
    for img_path, label in zip(valid_images, labels):
        bg_clusters[label].append(img_path)
    
    # æ‰“å°èšç±»ç»“æœ
    for cluster_id, images in bg_clusters.items():
        cluster_features = features[labels == cluster_id]
        avg_brightness = np.mean(cluster_features[:, 0])
        print(f"  ç±»åˆ« {cluster_id}: {len(images)} å¼ å›¾åƒ, å¹³å‡äº®åº¦={avg_brightness:.1f}")
    
    return dict(bg_clusters), features, labels

def check_brightness_compatibility(target_region, dust_brightness, threshold=15):
    """æ£€æŸ¥ç›®æ ‡åŒºåŸŸäº®åº¦æ˜¯å¦ä¸ç°å°˜åŸèƒŒæ™¯å…¼å®¹"""
    target_brightness = compute_brightness(target_region)
    return abs(target_brightness - dust_brightness) < threshold

def generate_synthetic():
    dust_metadata = load_dust_samples_and_stats()
    if not dust_metadata:
        print("âŒ æœªæ‰¾åˆ°ç°å°˜æ ·æœ¬ï¼è¯·æ£€æŸ¥è·¯å¾„ã€‚")
        return

    bg_images = glob.glob(os.path.join(NO_DUST_IMAGES_DIR, "*.*"))
    bg_images = [f for f in bg_images if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp'))]
    
    if len(bg_images) == 0:
        print(f"âŒ æœªæ‰¾åˆ°èƒŒæ™¯å›¾åƒï¼è·¯å¾„: {NO_DUST_IMAGES_DIR}")
        return
    
    print(f"\n{'='*60}")
    print(f"ğŸš€ å¼€å§‹åˆæˆ - å·¥ä¸šçº§æ•°æ®å¢å¼º")
    print(f"{'='*60}")
    print(f"èƒŒæ™¯å›¾åƒ: {len(bg_images)} å¼ ")
    print(f"ç°å°˜æ ·æœ¬: {len(dust_metadata)} ä¸ª")
    print(f"\nå¯ç”¨çš„ä¼˜åŒ–ç­–ç•¥:")
    print(f"  âœ“ äº®åº¦åŸŸåŒ¹é…: {USE_BRIGHTNESS_MATCHING} (é˜ˆå€¼={BRIGHTNESS_THRESHOLD})")
    print(f"  âœ“ ç©ºé—´ä½ç½®é”šå®š: {USE_SPATIAL_ANCHORING} (åŠå¾„={SPATIAL_RADIUS_RATIO})")
    print(f"  âœ“ èƒŒæ™¯èšç±»: {USE_BACKGROUND_CLUSTERING} (ç±»åˆ«æ•°={NUM_BG_CLUSTERS})")
    print(f"  âœ“ æ®‹å·®èåˆ: {USE_RESIDUAL_FUSION}")
    
    # èƒŒæ™¯èšç±»é¢„å¤„ç†
    bg_clusters = None
    bg_features = None
    bg_labels = None
    dust_cluster_map = None
    
    if USE_BACKGROUND_CLUSTERING:
        bg_clusters, bg_features, bg_labels = cluster_backgrounds(bg_images, n_clusters=NUM_BG_CLUSTERS)
        # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„å¹³å‡äº®åº¦ä½œä¸ºåŒ¹é…ç´¢å¼•
        dust_cluster_map = {}
        for cluster_id in range(NUM_BG_CLUSTERS):
            cluster_features = bg_features[bg_labels == cluster_id]
            dust_cluster_map[cluster_id] = np.mean(cluster_features[:, 0])
        bg_images_to_use = bg_clusters
    else:
        bg_images_to_use = bg_images
    
    print(f"\nå¼€å§‹åˆæˆ...\n")
    
    # ç»Ÿè®¡è®¡æ•°å™¨
    total_generated = 0
    brightness_rejected = 0
    position_adjusted = 0
    
    # éå†æ‰€æœ‰èƒŒæ™¯å›¾åƒ
    all_bg_paths = []
    if isinstance(bg_images_to_use, dict):
        for imgs in bg_images_to_use.values():
            all_bg_paths.extend(imgs)
    else:
        all_bg_paths = bg_images_to_use
    
    for bg_path in tqdm(all_bg_paths, desc="åˆæˆè¿›åº¦"):
        bg_img = cv_imread(bg_path)
        if bg_img is None:
            continue
            
        H, W = bg_img.shape[:2]
        new_labels = []
        
        # Clone background for blending
        synthetic_img = bg_img.astype(np.float32)
        
        # Randomly decide how many dusts to paste
        num_dusts = random.randint(1, 2)
        
        success_count = 0
        attempts_total = 0
        MAX_TOTAL_ATTEMPTS = 100

        while success_count < num_dusts and attempts_total < MAX_TOTAL_ATTEMPTS:
            attempts_total += 1
            
            # ä»å…ƒæ•°æ®ä¸­é€‰æ‹©ç°å°˜æ ·æœ¬
            dust_meta = random.choice(dust_metadata)
            patch = dust_meta['patch'].copy()
            
            # --- Augmentation: Random Scaling ---
            scale = random.uniform(0.7, 1.3)
            ph, pw = patch.shape[:2]
            new_h, new_w = int(ph * scale), int(pw * scale)
            
            # Constrain to max 1/4 of background
            if new_h > H // 4 or new_w > W // 4:
                scale_limit = min((H // 4) / ph, (W // 4) / pw)
                new_h, new_w = int(ph * scale_limit), int(pw * scale_limit)
            
            # Skip if too small
            if new_h < 5 or new_w < 5:
                continue
            
            patch_resized = cv2.resize(patch, (new_w, new_h))
            ph, pw = patch_resized.shape[:2]
            
            # --- Augmentation: Random Rotation ---
            k = random.randint(0, 3)
            patch_resized = np.rot90(patch_resized, k).copy()
            ph, pw = patch_resized.shape[:2]
            
            # --- ç©ºé—´ä½ç½®é”šå®šç­–ç•¥ ---
            if USE_SPATIAL_ANCHORING:
                # ä½¿ç”¨åŸå§‹ä½ç½®ä½œä¸ºå‚è€ƒç‚¹
                ref_x, ref_y = dust_meta['position']
                
                # è®¡ç®—å…è®¸çš„åç§»åŠå¾„ï¼ˆåƒç´ ï¼‰
                spatial_radius = int(W * SPATIAL_RADIUS_RATIO)
                
                # åœ¨åŠå¾„å†…éšæœºåç§»
                offset_x = random.randint(-spatial_radius, spatial_radius)
                offset_y = random.randint(-spatial_radius, spatial_radius)
                
                center_x = int(ref_x * W) + offset_x
                center_y = int(ref_y * H) + offset_y
                
                position_adjusted += 1
            else:
                # å›é€€åˆ°é«˜æ–¯æŠ–åŠ¨ç­–ç•¥
                ref_x, ref_y = dust_meta['position']
                jitter_std = 0.1
                norm_cx = ref_x + random.gauss(0, jitter_std)
                norm_cy = ref_y + random.gauss(0, jitter_std)
                norm_cx = max(0.0, min(1.0, norm_cx))
                norm_cy = max(0.0, min(1.0, norm_cy))
                center_x = int(norm_cx * W)
                center_y = int(norm_cy * H)
            
            # Calculate top-left from center
            x1 = center_x - pw // 2
            y1 = center_y - ph // 2
            x2 = x1 + pw
            y2 = y1 + ph
            
            # Boundary check and adjustment
            if x1 < 0:
                x1 = 0
                x2 = pw
            if y1 < 0:
                y1 = 0
                y2 = ph
            if x2 > W:
                x2 = W
                x1 = W - pw
            if y2 > H:
                y2 = H
                y1 = H - ph
            
            # Final validation
            if x1 < 0 or y1 < 0 or x2 > W or y2 > H:
                continue
            
            # --- äº®åº¦åŸŸåŒ¹é…æ£€æŸ¥ ---
            if USE_BRIGHTNESS_MATCHING:
                target_region = bg_img[y1:y2, x1:x2]
                if not check_brightness_compatibility(
                    target_region, 
                    dust_meta['brightness'], 
                    threshold=BRIGHTNESS_THRESHOLD
                ):
                    brightness_rejected += 1
                    continue
            
            # Recalculate final center
            final_cx = x1 + pw // 2
            final_cy = y1 + ph // 2
            
            # --- Blending Strategy: Alpha Blending ---
            feather_size = max(2, int(min(ph, pw) * 0.1))
            alpha_mask = create_soft_mask(ph, pw, feather_size)
            alpha_mask_3c = np.stack([alpha_mask] * 3, axis=-1)
            
            # Get ROI from background
            roi = synthetic_img[y1:y2, x1:x2]
            
            # Alpha blend
            patch_float = patch_resized.astype(np.float32)
            blended = patch_float * alpha_mask_3c + roi * (1.0 - alpha_mask_3c)
            
            # Paste blended result
            synthetic_img[y1:y2, x1:x2] = blended
            
            # --- Generate Label ---
            cx = final_cx / W
            cy = final_cy / H
            nw = pw / W
            nh = ph / H
            
            new_labels.append(f"0 {cx:.6f} {cy:.6f} {nw:.6f} {nh:.6f}")
            success_count += 1
        
        # Check if we failed to paste any dust
        if success_count == 0:
            continue
                
        # Save new image and label
        basename = os.path.basename(bg_path)
        name, _ = os.path.splitext(basename)
        new_filename = f"syn_{name}.jpg"
        
        # Convert back to uint8
        final_img = synthetic_img.clip(0, 255).astype(np.uint8)
        
        cv_imwrite(os.path.join(OUTPUT_IMAGES_DIR, new_filename), final_img)
        
        with open(os.path.join(OUTPUT_LABELS_DIR, f"syn_{name}.txt"), 'w') as f:
            f.write('\n'.join(new_labels))
        
        total_generated += 1
    
    print(f"\n{'='*60}")
    print(f"âœ… åˆæˆå®Œæˆï¼")
    print(f"{'='*60}")
    print(f"ç”Ÿæˆå›¾åƒ: {total_generated} å¼ ")
    print(f"è¾“å‡ºè·¯å¾„: {OUTPUT_DIR}")
    if USE_BRIGHTNESS_MATCHING:
        print(f"äº®åº¦åŒ¹é…æ‹’ç»: {brightness_rejected} æ¬¡")
    if USE_SPATIAL_ANCHORING:
        print(f"ä½ç½®é”šå®šåº”ç”¨: {position_adjusted} æ¬¡")
    print(f"\nğŸ’¡ æç¤º: æ£€æŸ¥ {os.path.join(OUTPUT_DIR, 'debug_patches')} æŸ¥çœ‹æå–çš„æ ·æœ¬")

if __name__ == "__main__":
    generate_synthetic()
