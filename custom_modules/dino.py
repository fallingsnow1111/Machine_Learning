import torch
from torch import nn
import torch.nn.functional as F
from modelscope import AutoModel
import numpy as np


class DINO3Preprocessor(nn.Module):
    """
    DINO3 Preprocessor - åœ¨P0è¾“å…¥é˜¶æ®µå¢å¼ºå›¾åƒ
    
    æ¶æ„: Input Image (3ch) -> DINO3ç‰¹å¾æå– -> å·ç§¯ç½‘ç»œ -> Enhanced Image (3ch)
    è¾“å‡ºå¢å¼ºçš„RGBå›¾åƒï¼Œè€Œéç‰¹å¾å‘é‡
    
    Args:
        c1: è¾“å…¥é€šé“æ•°ï¼ˆYOLO è‡ªåŠ¨ä¼ å…¥ï¼Œé€šå¸¸æ˜¯ 3ï¼‰
        model_name_or_path: DINO æ¨¡å‹è·¯å¾„æˆ–åç§°
        output_channels: è¾“å‡ºé€šé“æ•°ï¼ˆé»˜è®¤ 3ï¼‰
    """
    def __init__(self, c1, model_name_or_path='facebook/dinov3-vitl16-pretrain-lvd1689m', output_channels=3):
        super().__init__()
        self.c1 = c1
        self.model_name = model_name_or_path
        self.output_channels = output_channels
        
        # ä» modelscope åŠ è½½ DINO æ¨¡å‹
        print(f"ğŸ“¥ DINO3Preprocessor æ­£åœ¨ä»è·¯å¾„åŠ è½½æ¨¡å‹: {model_name_or_path}")
        print(f"   è¾“å…¥é€šé“: {c1}, è¾“å‡ºé€šé“: {output_channels}")
        self.dino = AutoModel.from_pretrained(model_name_or_path, trust_remote_code=True)
        self.embed_dim = self.dino.config.hidden_size  # 1024 for vitl16
        self.patch_size = self.dino.config.patch_size  # 16
        
        # ç‰¹å¾å¤„ç†ç½‘ç»œ: DINOç‰¹å¾ -> 3é€šé“å¢å¼ºå›¾åƒ
        # å‚è€ƒä»“åº“: é€šè¿‡å·ç§¯ç½‘ç»œå°†é«˜ç»´ç‰¹å¾è½¬æ¢ä¸º3é€šé“å›¾åƒ
        self.feature_processor = nn.Sequential(
            nn.Conv2d(self.embed_dim, 512, 3, padding=1),
            nn.BatchNorm2d(512),
            nn.SiLU(inplace=True),
            nn.Conv2d(512, 256, 3, padding=1),
            nn.BatchNorm2d(256),
            nn.SiLU(inplace=True),
            nn.Conv2d(256, 64, 3, padding=1),
            nn.BatchNorm2d(64),
            nn.SiLU(inplace=True),
            nn.Conv2d(64, self.output_channels, 3, padding=1),
            nn.Tanh()  # è¾“å‡ºå½’ä¸€åŒ–åˆ° [-1, 1]
        )
        
        # æ®‹å·®è¿æ¥æƒé‡
        self.gamma = nn.Parameter(torch.zeros(1))
        
        print(f"âœ… DINO3Preprocessor åˆå§‹åŒ–å®Œæˆ")
        print(f"   ç‰¹å¾ç»´åº¦: {self.embed_dim}, è¾“å‡ºé€šé“: {self.output_channels}")
    
    def forward(self, x):
        """
        Args:
            x: [B, 3, H, W] è¾“å…¥å›¾åƒ
        Returns:
            enhanced_image: [B, 3, H, W] å¢å¼ºåçš„å›¾åƒ
        """
        B, C, H, W = x.shape
        device = x.device
        original_input = x
        
        # DINO æœŸæœ›è¾“å…¥: [B, 3, 1024, 1024]
        x_resized = F.interpolate(x, size=(1024, 1024), mode='bilinear', align_corners=False)
        mean = torch.tensor([0.485, 0.456, 0.406], device=x.device).view(1, 3, 1, 1)
        std = torch.tensor([0.229, 0.224, 0.225], device=x.device).view(1, 3, 1, 1)
        x_normalized = (x_resized - mean) / std
        
        # æå– DINO ç‰¹å¾
        with torch.no_grad():
            outputs = self.dino(pixel_values=x_normalized, output_hidden_states=True)
            last_hidden_state = outputs.hidden_states[-1]  # [B, num_tokens, embed_dim]
        
        # å»æ‰ [CLS] token å’Œ register tokens
        num_registers = 4
        spatial_features = last_hidden_state[:, 1 + num_registers:, :]  # [B, num_patches, embed_dim]
        
        # é‡å¡‘ä¸ºç©ºé—´ç‰¹å¾å›¾
        _, num_patches, _ = spatial_features.shape
        h = w = int(np.sqrt(num_patches))
        dino_features = spatial_features.permute(0, 2, 1).reshape(B, self.embed_dim, h, w)
        
        # é€šè¿‡ç‰¹å¾å¤„ç†ç½‘ç»œè½¬æ¢ä¸º3é€šé“å›¾åƒ
        enhanced_features = self.feature_processor(dino_features)
        
        # ä¸Šé‡‡æ ·åˆ°åŸå§‹å°ºå¯¸
        enhanced_features = F.interpolate(
            enhanced_features, size=(H, W), mode='bilinear', align_corners=False
        )
        
        # Tanhè¾“å‡ºæ˜¯ [-1, 1]ï¼Œå½’ä¸€åŒ–åˆ° [0, 1]
        enhanced_features = (enhanced_features + 1) / 2
        
        # ä¸åŸå›¾åŠ æƒæ®‹å·®è¿æ¥
        enhanced_image = (
            original_input * (1 - self.gamma) + 
            enhanced_features * self.gamma
        )
        
        return enhanced_image


class DINO3Backbone(nn.Module):
    """
    DINO3 Backbone - åœ¨P3é˜¶æ®µå¢å¼ºCNNç‰¹å¾
    
    æ¶æ„: CNN Features -> æŠ•å½±ä¸ºä¼ªRGB -> DINO3ç‰¹å¾æå– -> ä¸åŸCNNç‰¹å¾èåˆ
    
    Args:
        c1: è¾“å…¥é€šé“æ•°ï¼ˆYOLO è‡ªåŠ¨ä¼ å…¥ï¼Œå¦‚ P3 å±‚çš„ 512 é€šé“ï¼‰
        model_name_or_path: DINO æ¨¡å‹è·¯å¾„æˆ–åç§°
        output_channels: è¾“å‡ºé€šé“æ•°ï¼ˆå¦‚ 128ï¼‰
    """
    def __init__(self, c1, model_name_or_path='facebook/dinov3-vits16-pretrain-lvd1689m', 
                 output_channels=512):
        super().__init__()
        self.c1 = c1  # ä¿å­˜è¾“å…¥é€šé“æ•°
        self.model_name = model_name_or_path
        self.output_channels = output_channels
        
        # ä» modelscope åŠ è½½ DINO æ¨¡å‹
        print(f"ğŸ“¥ DINO3Backbone æ­£åœ¨ä»è·¯å¾„åŠ è½½æ¨¡å‹: {model_name_or_path}")
        print(f"   è¾“å…¥é€šé“: {c1}, è¾“å‡ºé€šé“: {output_channels}")
        self.dino = AutoModel.from_pretrained(model_name_or_path, trust_remote_code=True)
        self.embed_dim = self.dino.config.hidden_size  # 1024 for vitl16
        self.patch_size = self.dino.config.patch_size  # 16

        
        # æŠ•å½±å±‚å°†åœ¨ç¬¬ä¸€æ¬¡forwardæ—¶åŠ¨æ€åˆ›å»ºï¼ˆå› ä¸ºinput_channelså¯èƒ½æœªçŸ¥ï¼‰
        self.input_projection = None
        self.fusion_layer = None
        self.feature_adapter = None
        self.spatial_projection = None
        
        print(f"âœ… DINO3Backbone åˆå§‹åŒ–å®Œæˆ")
        print(f"   ç‰¹å¾ç»´åº¦: {self.embed_dim}, è¾“å‡ºé€šé“: {self.output_channels}")
    
    def _create_projection_layers(self, input_channels=None):
        """æ ¹æ®å®é™…è¾“å…¥é€šé“æ•°åˆ›å»ºæŠ•å½±å±‚"""
        # å¦‚æœæ²¡æœ‰ä¼ å…¥ input_channelsï¼Œä½¿ç”¨ self.c1
        if input_channels is None:
            input_channels = self.c1
        
        # CNNç‰¹å¾ -> ä¼ªRGB (ç”¨äºé€å…¥DINO)
        self.input_projection = nn.Sequential(
            nn.Conv2d(input_channels, 64, 3, 1, 1),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 3, 1, 1),
            nn.Tanh()
        )
        
        # DINOç‰¹å¾é€‚é…å™¨: embed_dim -> output_channels
        self.feature_adapter = nn.Sequential(
            nn.Linear(self.embed_dim, self.output_channels),
            nn.LayerNorm(self.output_channels),
            nn.GELU()
        )
        
        # ç©ºé—´æŠ•å½±: è°ƒæ•´ç‰¹å¾å›¾åˆ†è¾¨ç‡
        self.spatial_projection = nn.Sequential(
            nn.Conv2d(self.output_channels, self.output_channels, 3, 1, 1),
            nn.BatchNorm2d(self.output_channels),
            nn.ReLU(inplace=True)
        )
        
        # èåˆå±‚: CNNç‰¹å¾ + DINOç‰¹å¾
        self.fusion_layer = nn.Sequential(
            nn.Conv2d(input_channels + self.output_channels, self.output_channels, 3, 1, 1),
            nn.BatchNorm2d(self.output_channels),
            nn.ReLU(inplace=True)
        )
    
    def forward(self, x):
        """
        Args:
            x: [B, C, H, W] CNNç‰¹å¾ (å¦‚P3å±‚çš„256é€šé“ç‰¹å¾)
        Returns:
            enhanced_features: [B, output_channels, H, W] å¢å¼ºåçš„ç‰¹å¾
        """
        B, C, H, W = x.shape
        device = x.device
        
        # ç¬¬ä¸€æ¬¡forwardæ—¶åˆ›å»ºæŠ•å½±å±‚
        if self.input_projection is None:
            self.input_channels = C
            self._create_projection_layers(C)
            # ç§»åŠ¨åˆ°ä¸è¾“å…¥ç›¸åŒçš„è®¾å¤‡
            self.input_projection = self.input_projection.to(device)
            self.feature_adapter = self.feature_adapter.to(device)
            self.spatial_projection = self.spatial_projection.to(device)
            self.fusion_layer = self.fusion_layer.to(device)
        
        # 1. å°†CNNç‰¹å¾æŠ•å½±ä¸ºä¼ªRGBå›¾åƒ
        pseudo_rgb = self.input_projection(x)  # [B, 3, H, W]
        
        # 2. è°ƒæ•´åˆ°DINOæœŸæœ›çš„å°ºå¯¸
        dino_size = 224  # DINOè®­ç»ƒæ—¶çš„æ ‡å‡†å°ºå¯¸
        pseudo_rgb_resized = F.interpolate(
            pseudo_rgb, size=(dino_size, dino_size), 
            mode='bilinear', align_corners=False
        )
        
        # ImageNet å½’ä¸€åŒ–
        mean = torch.tensor([0.485, 0.456, 0.406], device=device).view(1, 3, 1, 1)
        std = torch.tensor([0.229, 0.224, 0.225], device=device).view(1, 3, 1, 1)
        pseudo_rgb_normalized = (pseudo_rgb_resized - mean) / std
        
        # 3. é€šè¿‡DINOæå–ç‰¹å¾
        with torch.no_grad():
            outputs = self.dino(pixel_values=pseudo_rgb_normalized, output_hidden_states=True)
            last_hidden_state = outputs.hidden_states[-1]  # [B, num_tokens, embed_dim]
        
        # å»æ‰ [CLS] token å’Œ register tokens
        num_registers = 4
        spatial_features = last_hidden_state[:, 1 + num_registers:, :]  # [B, num_patches, embed_dim]
        
        # é‡å¡‘ä¸ºç©ºé—´ç‰¹å¾å›¾
        _, num_patches, _ = spatial_features.shape
        h = w = int(np.sqrt(num_patches))
        
        # 4. é€‚é…é€šé“ç»´åº¦
        # [B, num_patches, embed_dim] -> [B, h, w, embed_dim]
        features_2d = spatial_features.view(B, h, w, self.embed_dim)
        # é€šè¿‡çº¿æ€§å±‚é€‚é…: embed_dim -> output_channels
        adapted_features = self.feature_adapter(features_2d)  # [B, h, w, output_channels]
        # è½¬æ¢ä¸º [B, output_channels, h, w]
        adapted_features = adapted_features.permute(0, 3, 1, 2)
        
        # 5. ç©ºé—´æŠ•å½±å’Œä¸Šé‡‡æ ·åˆ°åŸå§‹å°ºå¯¸
        dino_features = self.spatial_projection(adapted_features)
        dino_features_resized = F.interpolate(
            dino_features, size=(H, W), 
            mode='bilinear', align_corners=False
        )
        
        # 6. ä¸åŸCNNç‰¹å¾èåˆ
        combined_features = torch.cat([x, dino_features_resized], dim=1)
        enhanced_features = self.fusion_layer(combined_features)
        
        return enhanced_features