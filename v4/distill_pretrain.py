"""
DINOè’¸é¦é¢„è®­ç»ƒä¸»è„šæœ¬
åŠŸèƒ½ï¼šä½¿ç”¨DINOv3ä½œä¸ºæ•™å¸ˆæ¨¡å‹ï¼Œå¯¹YOLO11néª¨å¹²ç½‘ç»œè¿›è¡ŒçŸ¥è¯†è’¸é¦
"""

import sys
import os
from pathlib import Path
from tqdm import tqdm
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
import torchvision.transforms as transforms
from PIL import Image
from modelscope import AutoModel

# å¯¼å…¥é…ç½®æ–‡ä»¶
from distill_config import *

# ===================== é¡¹ç›®æ ¹ç›®å½•é…ç½® =====================
try:
    PROJECT_ROOT = Path(__file__).parent.parent
except NameError:
    PROJECT_ROOT = Path.cwd()

sys.path.insert(0, str(PROJECT_ROOT))
print(f"ğŸ“‚ é¡¹ç›®æ ¹ç›®å½•: {PROJECT_ROOT}")

# ===================== å·¥å…·å‡½æ•° =====================
def find_model_config_dir(base_dir):
    """é€’å½’æŸ¥æ‰¾åŒ…å«config.jsonçš„æ¨¡å‹ç›®å½•"""
    base_dir = Path(base_dir)
    print(f"ğŸ” æœç´¢æ¨¡å‹æ ¸å¿ƒæ–‡ä»¶: {base_dir}")
    
    for config_path in base_dir.rglob("config.json"):
        model_dir = config_path.parent
        has_safetensors = (model_dir / "model.safetensors").exists()
        has_bin = (model_dir / "pytorch_model.bin").exists()
        
        if has_safetensors or has_bin:
            print(f"âœ… æ‰¾åˆ°æ¨¡å‹ç›®å½•: {model_dir}")
            return model_dir
    
    print(f"âŒ æœªæ‰¾åˆ°æœ‰æ•ˆæ¨¡å‹ç›®å½•")
    return None

# ===================== æ•°æ®é›†ç±» =====================
class SimpleImageDataset(torch.utils.data.Dataset):
    """ç®€å•çš„å›¾åƒæ•°æ®é›†ï¼ˆæ— æ ‡ç­¾ï¼‰"""
    def __init__(self, image_dir, transform=None):
        self.image_dir = Path(image_dir)
        self.image_files = list(self.image_dir.glob("*.jpg")) + \
                          list(self.image_dir.glob("*.png")) + \
                          list(self.image_dir.glob("*.jpeg"))
        self.transform = transform
        print(f"ğŸ“¦ åŠ è½½ {len(self.image_files)} å¼ å›¾åƒ")
    
    def __len__(self):
        return len(self.image_files)
    
    def __getitem__(self, idx):
        img_path = self.image_files[idx]
        try:
            image = Image.open(img_path).convert('RGB')
            if self.transform:
                image = self.transform(image)
            return image
        except Exception as e:
            print(f"âš ï¸ åŠ è½½å›¾åƒå¤±è´¥: {img_path}, ä½¿ç”¨éšæœºå¼ é‡æ›¿ä»£")
            return torch.randn(3, IMG_SIZE, IMG_SIZE)

# ===================== YOLOéª¨å¹²ç½‘ç»œæå–å™¨ =====================
class YOLO11BackboneExtractor(nn.Module):
    """ä»YOLO11å®Œæ•´æ¨¡å‹ä¸­æå–éª¨å¹²ç½‘ç»œå¹¶æ·»åŠ é€‚é…å™¨"""
    def __init__(self, yolo_wrapper, layer_idx=BACKBONE_LAYER_IDX):
        super().__init__()
        
        # æå–YOLO11çš„éª¨å¹²ç½‘ç»œ
        if hasattr(yolo_wrapper.model, 'model'):
            full_model = yolo_wrapper.model.model
        else:
            full_model = yolo_wrapper.model
        
        self.backbone = nn.Sequential(*list(full_model[:layer_idx]))
        
        # æ·»åŠ é€‚é…å™¨ï¼ˆç‰¹å¾å›¾ -> ç‰¹å¾å‘é‡ï¼‰
        self.adapter = nn.Sequential(
            nn.AdaptiveAvgPool2d((1, 1)),
            nn.Flatten(),
            nn.Linear(256, ADAPTER_HIDDEN_DIM)
        )
        
        print(f"âœ… éª¨å¹²ç½‘ç»œæå–æˆåŠŸï¼ˆå‰{layer_idx}å±‚ï¼‰")
    
    def forward(self, x):
        feat_map = self.backbone(x)        # ç‰¹å¾å›¾
        feat_vec = self.adapter(feat_map)  # ç‰¹å¾å‘é‡
        return feat_map, feat_vec

# ===================== DINOv3æ•™å¸ˆæ¨¡å‹ =====================
class DINOv3Teacher(nn.Module):
    """DINOv3æ•™å¸ˆæ¨¡å‹ï¼ˆViT-L/16ï¼‰- ä»ModelScopeç›´æ¥ä¸‹è½½"""
    def __init__(self, model_name="facebook/dinov3-vitl16-pretrain-lvd1689m"):
        super().__init__()
        
        print(f"ğŸ“¥ å‡†å¤‡åŠ è½½DINOv3 Teacher: {model_name}")
        
        try:
            # ç›´æ¥ä»ModelScopeä¸‹è½½æ¨¡å‹ï¼Œæ— éœ€è§£å‹tar.gz
            self.teacher = AutoModel.from_pretrained(
                model_name, 
                trust_remote_code=True,
                device_map="auto"  # è‡ªåŠ¨åˆ†é…åˆ°å¯ç”¨è®¾å¤‡
            )
            self.teacher.eval()
            
            # å†»ç»“æ•™å¸ˆæ¨¡å‹
            for param in self.teacher.parameters():
                param.requires_grad = False
            
            print(f"âœ… DINOv3 TeacheråŠ è½½æˆåŠŸ")
            
        except Exception as e:
            print(f"âŒ åŠ è½½å¤±è´¥: {e}")
            raise Exception(f"æ— æ³•ä»ModelScopeåŠ è½½DINOv3: {e}")
    
    def forward(self, x):
        """
        å‰å‘ä¼ æ’­ - æå–å®Œæ•´çš„ patch tokens ç‰¹å¾
        Args:
            x: è¾“å…¥å›¾åƒ [B, 3, H, W]
        Returns:
            feat_map: ç‰¹å¾å›¾ [B, D, H', W']ï¼ˆé‡å¡‘åçš„ patch tokensï¼‰
            feat_vec: ç‰¹å¾å‘é‡ [B, D]ï¼ˆå…¨å±€å¹³å‡æ± åŒ–ï¼‰
        """
        B = x.shape[0]
        
        with torch.no_grad():
            outputs = self.teacher(pixel_values=x, output_hidden_states=True)
            last_hidden = outputs.hidden_states[-2]  # [B, num_tokens, D]
            
            # DINOv3ç»“æ„: [CLS(1)] + [Registers(4)] + [Patch Tokens(N-5)]
            # è·³è¿‡ CLS å’Œ registersï¼Œæå–ç©ºé—´ patch tokens
            num_registers = 4
            spatial_tokens = last_hidden[:, 1 + num_registers:, :]  # [B, num_patches, D]
            
            # è·å–ç‰¹å¾ç»´åº¦
            D = spatial_tokens.shape[-1]
            num_patches = spatial_tokens.shape[1]
            
            # è®¡ç®—ç‰¹å¾å›¾çš„ç©ºé—´å°ºå¯¸ (num_patches = H' * W')
            # DINOv3 ä½¿ç”¨ 14x14 patchï¼Œæ‰€ä»¥å¯¹äº 640x640 è¾“å…¥ä¼šå¾—åˆ° ~45x45
            H_prime = int(num_patches ** 0.5)
            W_prime = H_prime if (H_prime * H_prime == num_patches) else int(num_patches ** 0.5) + 1
            
            # å°† tokens é‡å¡‘ä¸ºç‰¹å¾å›¾ [B, D, H', W']
            feat_map = spatial_tokens.reshape(B, H_prime, W_prime, D).permute(0, 3, 1, 2)  
            
            # å…¨å±€å¹³å‡æ± åŒ–å¾—åˆ°ç‰¹å¾å‘é‡ [B, D]
            feat_vec = spatial_tokens.mean(dim=1)  # [B, D]
            
            # ä¸ºäº†ä¸å­¦ç”Ÿæ¨¡å‹ç»´åº¦åŒ¹é…ï¼Œå¯èƒ½éœ€è¦é€‚é…
            # å­¦ç”Ÿæ¨¡å‹çš„ feat_vec ç»è¿‡çº¿æ€§å±‚åæ˜¯ [B, 1024]
            # è¿™é‡Œæˆ‘ä»¬ä¿æŒåŸå§‹ç»´åº¦ï¼Œåœ¨æŸå¤±è®¡ç®—æ—¶å¤„ç†ç»´åº¦å¯¹é½
        
        return feat_map, feat_vec

# ===================== æŸå¤±å‡½æ•° =====================
def compute_distill_loss(student_vec, teacher_vec, student_map, teacher_map=None):
    """
    è®¡ç®—è’¸é¦æŸå¤±ï¼ˆåŒæ—¶è€ƒè™‘ç‰¹å¾å‘é‡å’Œç‰¹å¾å›¾ï¼‰
    student_vec: å­¦ç”Ÿæ¨¡å‹ç‰¹å¾å‘é‡ [B, D_s]
    teacher_vec: æ•™å¸ˆæ¨¡å‹ç‰¹å¾å‘é‡ [B, D_t]
    student_map: å­¦ç”Ÿæ¨¡å‹ç‰¹å¾å›¾ [B, C_s, H_s, W_s]
    teacher_map: æ•™å¸ˆæ¨¡å‹ç‰¹å¾å›¾ [B, C_t, H_t, W_t]ï¼ˆå¯é€‰ï¼‰
    """
    
    # 1. ç‰¹å¾å‘é‡ç›¸ä¼¼åº¦æŸå¤±
    # å¤„ç†ç»´åº¦ä¸åŒ¹é…ï¼šå°†é«˜ç»´æŠ•å½±åˆ°ä½ç»´
    D_s = student_vec.shape[-1]
    D_t = teacher_vec.shape[-1]
    
    if D_s != D_t:
        # ç®€å•çš„æŠ•å½±æ–¹æ¡ˆï¼šæˆªæ–­æˆ–å¡«å……
        if D_s > D_t:
            student_vec_aligned = student_vec[:, :D_t]
        else:
            # å­¦ç”Ÿç»´åº¦æ›´å°ï¼Œç”¨å‡å€¼å¡«å……
            padding = torch.zeros(student_vec.shape[0], D_t - D_s, device=student_vec.device)
            student_vec_aligned = torch.cat([student_vec, padding], dim=1)
    else:
        student_vec_aligned = student_vec
    
    # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
    cos_sim = torch.nn.functional.cosine_similarity(student_vec_aligned, teacher_vec, dim=1).mean()
    vec_loss = (1 - cos_sim) * DISTILL_LOSS_WEIGHT
    
    # 2. ç‰¹å¾å›¾ç›¸ä¼¼åº¦æŸå¤±ï¼ˆå¯é€‰ï¼‰
    if teacher_map is not None:
        # å°†ç‰¹å¾å›¾æ‹‰å¹³ä¸ºå‘é‡è¿›è¡Œæ¯”è¾ƒ
        B, C_s, H_s, W_s = student_map.shape
        _, C_t, H_t, W_t = teacher_map.shape
        
        # è°ƒæ•´åˆ°ç›¸åŒå°ºå¯¸
        if (H_s, W_s) != (H_t, W_t):
            teacher_map_resized = torch.nn.functional.interpolate(
                teacher_map, size=(H_s, W_s), mode='bilinear', align_corners=False
            )
        else:
            teacher_map_resized = teacher_map
        
        # ç‰¹å¾å›¾åŒ¹é…æŸå¤±ï¼šè®¡ç®—ç‰¹å¾å›¾çš„ç›¸ä¼¼åº¦
        student_map_flat = student_map.reshape(B, C_s, -1)  # [B, C, HW]
        teacher_map_flat = teacher_map_resized.reshape(B, C_t, -1)  # [B, C, HW]
        
        # è®¡ç®—æ¯ä¸ªåƒç´ ç‚¹çš„ç‰¹å¾ç›¸ä¼¼åº¦
        sim = torch.nn.functional.cosine_similarity(student_map_flat, teacher_map_flat, dim=1)  # [B, HW]
        map_loss = (1 - sim.mean()) * 0.5 * DISTILL_LOSS_WEIGHT
    else:
        map_loss = 0
    
    # 3. ç‰¹å¾å¤šæ ·æ€§æŸå¤±ï¼ˆé¼“åŠ±ç‰¹å¾å¤šæ ·æ€§ï¼‰
    feat_flat = student_map.reshape(B, student_map.shape[1], -1)
    var_loss = -torch.var(feat_flat, dim=[0, 2]).mean() * VAR_LOSS_WEIGHT
    
    total_loss = vec_loss + map_loss + var_loss
    
    return total_loss

def compute_simplified_loss(student_vec, student_map):
    """ç®€åŒ–æŸå¤±ï¼ˆæ— æ•™å¸ˆæ¨¡å‹æ—¶ä½¿ç”¨ï¼‰"""
    B, D = student_vec.shape
    
    # ç‰¹å¾æ–¹å·®æŸå¤±
    vec_var = torch.var(student_vec, dim=0).mean()
    var_loss = -vec_var * VAR_LOSS_WEIGHT
    
    # å½’ä¸€åŒ–æŸå¤±
    norm_loss = torch.abs(student_vec.norm(dim=1) - 1.0).mean() * NORM_LOSS_WEIGHT
    
    return var_loss + norm_loss

# ===================== ä¸»è®­ç»ƒæµç¨‹ =====================
def run_distillation():
    """æ‰§è¡Œè’¸é¦é¢„è®­ç»ƒ"""
    
    # è·¯å¾„é…ç½®
    data_dir = Path(DISTILL_DATA_DIR)
    output_dir = Path(OUTPUT_DIR)
    yolo_weights = Path(YOLO11N_WEIGHTS)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    print("\n" + "="*60)
    print("ğŸš€ DINO â†’ YOLO11n è’¸é¦é¢„è®­ç»ƒ")
    print("="*60)
    print(f"ğŸ“ æ•°æ®ç›®å½•: {data_dir}")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
    print(f"ğŸ“Š è®­ç»ƒè½®æ•°: {EPOCHS}")
    print(f"ğŸ“Š æ‰¹æ¬¡å¤§å°: {BATCH_SIZE}")
    print(f"ğŸ’» è®¾å¤‡: {DEVICE}")
    print(f"ğŸ“¦ YOLOæƒé‡: {yolo_weights}")
    print("="*60 + "\n")
    
    # æ£€æŸ¥æ•°æ®ç›®å½•
    if not data_dir.exists():
        print(f"âŒ æ•°æ®ç›®å½•ä¸å­˜åœ¨: {data_dir}")
        print(f"ğŸ’¡ è¯·å…ˆè¿è¡Œ: python prepare_distill_data.py")
        sys.exit(1)
    
    # æ£€æŸ¥YOLOæƒé‡
    if not yolo_weights.exists():
        print(f"âš ï¸ YOLOæƒé‡ä¸å­˜åœ¨ï¼Œæ­£åœ¨ä¸‹è½½...")
        from ultralytics import YOLO
        yolo_temp = YOLO("yolo11n.pt")
        yolo_weights.parent.mkdir(parents=True, exist_ok=True)
        yolo_temp.save(str(yolo_weights))
    
    # åŠ è½½YOLOéª¨å¹²ç½‘ç»œ
    print("ğŸ“¦ åŠ è½½YOLO11néª¨å¹²ç½‘ç»œ...")
    from ultralytics import YOLO
    yolo_wrapper = YOLO(str(yolo_weights))
    student = YOLO11BackboneExtractor(yolo_wrapper, layer_idx=BACKBONE_LAYER_IDX).to(DEVICE)
    
    # å¤šGPUå¹¶è¡Œ
    gpu_count = torch.cuda.device_count()
    if gpu_count >= 2 and USE_DATAPARALLEL:
        student = nn.DataParallel(student)
        print(f"ğŸš€ å¯ç”¨åŒå¡/å¤šå¡å¹¶è¡Œ ({gpu_count} GPUs)")
    
    # åŠ è½½æ•™å¸ˆæ¨¡å‹
    print("ğŸ“¦ åŠ è½½DINOv3 Teacher...")
    teacher = None
    try:
        teacher = DINOv3Teacher().to(DEVICE)
        print("âœ… æ•™å¸ˆæ¨¡å‹åŠ è½½æˆåŠŸ")
    except Exception as e:
        print(f"âš ï¸ æ•™å¸ˆæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        print("å°†ä½¿ç”¨ç®€åŒ–æŸå¤±å‡½æ•°è¿›è¡Œé¢„è®­ç»ƒ")
    
    # å‡†å¤‡æ•°æ®
    print("ğŸ“¦ å‡†å¤‡æ•°æ®...")
    transform = transforms.Compose([
        transforms.Resize((IMG_SIZE, IMG_SIZE)),
        transforms.ToTensor(),
        transforms.Normalize(NORMALIZE_MEAN, NORMALIZE_STD)
    ])
    
    dataset = SimpleImageDataset(data_dir, transform=transform)
    dataloader = DataLoader(
        dataset, 
        batch_size=BATCH_SIZE, 
        shuffle=True, 
        num_workers=NUM_WORKERS,
        pin_memory=True if DEVICE == "cuda" else False
    )
    
    if len(dataset) == 0:
        print(f"âŒ æ•°æ®é›†ä¸ºç©º: {data_dir}")
        sys.exit(1)
    
    print(f"âœ… åŠ è½½ {len(dataset)} å¼ å›¾åƒ")
    
    # ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨
    optimizer = optim.AdamW(student.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)
    
    # å¼€å§‹è®­ç»ƒ
    print("\nğŸš€ å¼€å§‹è’¸é¦è®­ç»ƒ...")
    student.train()
    
    for epoch in range(EPOCHS):
        total_loss = 0.0
        pbar = tqdm(dataloader, desc=f"Epoch {epoch+1}/{EPOCHS}", unit="batch")
        
        for batch_idx, images in enumerate(pbar):
            # è°ƒè¯•æ¨¡å¼ï¼šåªè®­ç»ƒå°‘é‡batch
            if DEBUG_MODE and batch_idx >= DEBUG_MAX_BATCHES:
                break
            
            images = images.to(DEVICE)
            
            # å‰å‘ä¼ æ’­
            student_map, student_vec = student(images)
            
            # è®¡ç®—æŸå¤±
            if teacher is not None:
                with torch.no_grad():
                    teacher_map, teacher_vec = teacher(images)
                loss = compute_distill_loss(student_vec, teacher_vec, student_map, teacher_map)
            else:
                loss = compute_simplified_loss(student_vec, student_map)
            
            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(student.parameters(), max_norm=1.0)
            optimizer.step()
            
            total_loss += loss.item()
            pbar.set_postfix({"loss": f"{loss.item():.4f}"})
        
        scheduler.step()
        avg_loss = total_loss / len(dataloader)
        
        # æ¯10è½®è¾“å‡ºä¸€æ¬¡
        if (epoch + 1) % 10 == 0:
            print(f"\nâœ… Epoch {epoch+1}/{EPOCHS} - Avg Loss: {avg_loss:.4f}")
        
        # ä¿å­˜æ£€æŸ¥ç‚¹
        if (epoch + 1) % SAVE_CHECKPOINT_INTERVAL == 0:
            checkpoint_path = output_dir / f"checkpoint_epoch{epoch+1}.pt"
            if isinstance(student, nn.DataParallel):
                torch.save(student.module.backbone.state_dict(), checkpoint_path)
            else:
                torch.save(student.backbone.state_dict(), checkpoint_path)
            print(f"ğŸ’¾ ä¿å­˜æ£€æŸ¥ç‚¹: {checkpoint_path}")
    
    # ä¿å­˜æœ€ç»ˆæƒé‡
    if SAVE_FINAL_WEIGHTS:
        print("\nğŸ”„ åˆå¹¶è’¸é¦æƒé‡åˆ°å®Œæ•´YOLOæ¨¡å‹...")
        final_weights = output_dir / "yolo11n_distilled.pt"
        
        # æå–éª¨å¹²ç½‘ç»œæƒé‡
        if isinstance(student, nn.DataParallel):
            backbone_state = student.module.backbone.state_dict()
        else:
            backbone_state = student.backbone.state_dict()
        
        # åŠ è½½å®Œæ•´YOLOæ¨¡å‹
        complete_model = YOLO(str(yolo_weights))
        if hasattr(complete_model.model, 'model'):
            full_model = complete_model.model.model
        else:
            full_model = complete_model.model
        
        model_state = full_model.state_dict()
        
        # æ˜ å°„æƒé‡
        print("ğŸ”„ æ˜ å°„è’¸é¦æƒé‡...")
        mapped_count = 0
        for key, val in backbone_state.items():
            if key in model_state:
                model_state[key] = val
                mapped_count += 1
        
        print(f"âœ“ æˆåŠŸæ˜ å°„ {mapped_count} ä¸ªæƒé‡å±‚")
        
        # ä¿å­˜
        full_model.load_state_dict(model_state, strict=False)
        complete_model.save(str(final_weights))
        
        print(f"\nâœ… è’¸é¦é¢„è®­ç»ƒå®Œæˆï¼")
        print(f"ğŸ“ å®Œæ•´æƒé‡ä¿å­˜: {final_weights}")
        print(f"\nğŸ’¡ ä¸‹ä¸€æ­¥ï¼š")
        print(f"   python train.py  # è‡ªåŠ¨åŠ è½½è’¸é¦æƒé‡è¿›è¡Œæ£€æµ‹è®­ç»ƒ")

if __name__ == "__main__":
    try:
        run_distillation()
    except Exception as e:
        print(f"\nâŒ è’¸é¦å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
